{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics #for accuracy calculation\n",
    "import os\n",
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import RSLPStemmer\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The final dataframes columns is: file_name, category and content\n",
    "def create_dataframe(path):\n",
    "    # ignore hidden files\n",
    "    categories = [f for f in os.listdir(path) if not f.startswith('.')]\n",
    "    dataset = []\n",
    "    for category in categories:\n",
    "        for abstract in os.listdir(path+category):\n",
    "            if not abstract.startswith(\".\"):\n",
    "                text = \"\"\n",
    "                row = []\n",
    "                with open(\"{}/{}\".format(path+category, abstract)) as f: \n",
    "                        for line in f: \n",
    "                            text += line \n",
    "                row.append(abstract) #file_name\n",
    "                row.append(category) #category\n",
    "                row.append(text)\n",
    "                dataset.append(row)          \n",
    "    return dataset\n",
    "\n",
    "def transform_to_pandas_dataframe(matrix, column_names=[\"file_name\", \"category\", \"content\"]):\n",
    "    return pd.DataFrame(matrix, columns = column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):  #input = string\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text): #input = list\n",
    "    stopword = set(stopwords.words('portuguese') + list(punctuation))\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def stemmer(text):\n",
    "    stemmer = RSLPStemmer()\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        new_text.append(stemmer.stem(word.lower()))\n",
    "    return new_text\n",
    "\n",
    "def remove_special_character(text):\n",
    "    text = normalize('NFKD', text).encode('ASCII','ignore').decode('ASCII')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remover acentuação\n",
    "- Colocar texto em minusculo\n",
    "- Transformar em tokens\n",
    "- Remover stop words\n",
    "- Lemmatizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = \"../data/raw/\"\n",
    "df = create_dataframe(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "text_position = 2\n",
    "category_position = 1\n",
    "\n",
    "ind_lower = True\n",
    "ind_remove_special_character = True\n",
    "ind_tokenizer = True\n",
    "ind_remove_stopwords = True\n",
    "ind_stemmer = True\n",
    "\n",
    "for row in df:\n",
    "    text = row[text_position]\n",
    "    category = row[category_position]\n",
    "    \n",
    "    if ind_lower:\n",
    "        # input type: string\n",
    "        text = text.lower()\n",
    "    \n",
    "    if ind_remove_special_character:\n",
    "        # input type: string\n",
    "        text = remove_special_character(text)\n",
    "    \n",
    "    if ind_tokenizer:\n",
    "        # input type: string\n",
    "        text = tokenizer(text)\n",
    "    \n",
    "    if ind_remove_stopwords:\n",
    "        # input type: list\n",
    "        text = remove_stopwords(text)\n",
    "        \n",
    "    if ind_stemmer:\n",
    "        # input type: list\n",
    "        text = stemmer(text)\n",
    "        \n",
    "    output.append([category, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15.txt',\n",
       " 'neurologia',\n",
       " 'A Doença de Parkinson é considerada uma patologia neurodegenerativa que afeta principalmente idosos, podendo ser manifestada de outras formas em indivíduos mais jovens, sendo caracterizada pela diminuição de produção de dopamina resultando em tremores involuntários, bradicinesia e perda de equilíbrio. O diagnóstico da doença é complexo e é realizado basicamente pelo quadro clínico do paciente. A detecção do Parkinson de forma precoce é um desafio relevante, o que gerou novos estudos e desenvolvimento de novas ferramentas de diagnóstico para prever a doença e impedir o seu avanço. As técnicas de imagem são exames importantes que podem ser aplicados para o estadiamento do indivíduo. Este trabalho consiste em uma revisão bibliográfica narrativa, com o objetivo de apresentar o uso de técnicas de medicina nuclear capazes de identificar a patologia de forma precoc']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neurologia', ['doenc', 'parkinson', 'consider', 'patolog', 'neurodegener', 'afet', 'princip', 'idos', 'pod', 'ser', 'manifest', 'outr', 'form', 'individu', 'jov', 'send', 'caracter', 'diminuica', 'produca', 'dopamin', 'result', 'trem', 'involuntari', 'bradicines', 'perd', 'equilibri', 'diagnost', 'doenc', 'complex', 'realiz', 'basic', 'quadr', 'clin', 'paci', 'detecca', 'parkinson', 'form', 'precoc', 'desafi', 'relev', 'ger', 'nov', 'estud', 'desenvolv', 'nov', 'ferrament', 'diagnost', 'prev', 'doenc', 'imped', 'avanc', 'tecn', 'imag', 'sao', 'exam', 'import', 'pod', 'ser', 'aplic', 'estad', 'individu', 'trabalh', 'cons', 'revisa', 'bibliograf', 'narr', 'obje', 'apresent', 'uso', 'tecn', 'medicin', 'nucle', 'capaz', 'identific', 'patolog', 'form', 'precoc']]\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     /Users/danielaalmeida/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "#nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n",
      "Precision: 0.9811320754716981\n",
      "Recall: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
